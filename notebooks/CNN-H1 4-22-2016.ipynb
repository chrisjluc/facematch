{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>CNN-H1 4-22-2016</h1>\n",
    "\n",
    "<strong>Abstract</strong>\n",
    "Implementing the CNN-H1 using NN2 described in the paper: http://arxiv.org/pdf/1509.00244v1.pdf. \n",
    "\n",
    "<strong>Implementation</strong>\n",
    "<ul>\n",
    "<li>Normalizes images to 230x230 centered around the facial features and then takes the middle 165X120 portion</li>\n",
    "<li>Validation split of 15%</li>\n",
    "<li>fc6 layer has a length of 512</li>\n",
    "<li>Noise with width 15</li>\n",
    "<li>Number of people is 600</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: GRID K520 (CNMeM is enabled with initial size: 98.0% of memory, CuDNN 3007)\n"
     ]
    }
   ],
   "source": [
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import dlib\n",
    "import os\n",
    "import fnmatch\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "from matplotlib.pyplot import imshow\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "from scipy.spatial import distance\n",
    "from scipy.misc import imrotate\n",
    "\n",
    "from skimage import io\n",
    "from skimage.color import rgb2grey\n",
    "from skimage.transform import resize\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D, AveragePooling2D\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import model_from_json\n",
    "\n",
    "np.random.seed(123456)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = '../data/lfw_cropped'\n",
    "\n",
    "input_shape = 165, 120\n",
    "noise_width = 15\n",
    "num_people = 600"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Loading Files</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_face_to_file_path_dict():\n",
    "    face_to_file_paths_dict = {}\n",
    "    \n",
    "    for root, dirnames, filenames in os.walk(data_path):\n",
    "        for dirname in dirnames:\n",
    "            if dirname not in face_to_file_paths_dict:\n",
    "                face_to_file_paths_dict[dirname] = []\n",
    "            directory_path = os.path.join(data_path, dirname)\n",
    "            for filename in os.listdir(directory_path):\n",
    "                if filename.endswith('_image.npy'):\n",
    "                    face_to_file_paths_dict[dirname].append(os.path.join(directory_path, filename))\n",
    "                            \n",
    "    return face_to_file_paths_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_face_to_file_paths_descending_list(face_to_file_paths_dict):\n",
    "    return sorted(face_to_file_paths_dict.items(), key=lambda x: len(x[1]), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "face_to_file_paths_dict = get_face_to_file_path_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "face_to_file_paths_list = get_face_to_file_paths_descending_list(face_to_file_paths_dict)[:num_people]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Helper Functions</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_image_window(image, size, point):\n",
    "    \"\"\"\n",
    "    Assume image is grey image\n",
    "    \"\"\"\n",
    "    top = int(point[1] - size[0] / 2)\n",
    "    left = int(point[0] - size[1] / 2)\n",
    "    return image[top:top + size[0], left:left + size[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def reflection(image):\n",
    "    return np.array([list(reversed(row)) for row in image])\n",
    "\n",
    "def partition(image, top_left, rows, cols):\n",
    "    return np.array([row[top_left[1]:top_left[1] + cols] for row in image[top_left[0]:top_left[0] + rows]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_random_noise_image(image, coords, width):\n",
    "    \"\"\"\n",
    "    Apply random gaussian generated values\n",
    "    and distribute them on gaussian distributed square\n",
    "    centered on the coordinates passed in for the image\n",
    "    \"\"\"\n",
    "    \n",
    "    noise = np.zeros((image.shape[0], image.shape[1]))\n",
    "    for coord in coords:\n",
    "        # Convert coordinates to rows / columns\n",
    "        apply_noise_at_point(coord[1], coord[0], noise, width)\n",
    "    return np.clip(image + noise, 0, 1)\n",
    "\n",
    "def apply_noise_at_point(x, y, noise, width):\n",
    "    \"\"\"\n",
    "    Generate a block with a single random value placed at the center\n",
    "    Apply the Gaussian filter with std of 4\n",
    "    Place it on the noise array at the appropriate coordinates\n",
    "    \n",
    "    x represents the rows\n",
    "    y represents the cols\n",
    "    \"\"\"\n",
    "    \n",
    "    block = np.zeros((width, width))\n",
    "    block[width / 2, width / 2] = np.random.normal()\n",
    "    block = gaussian_filter(block, sigma=4)\n",
    "    \n",
    "    x -= width / 2\n",
    "    y -= width / 2\n",
    "    \n",
    "    x_end = min(noise.shape[0] - x, block.shape[0])\n",
    "    x_start =  max(0, -x)\n",
    "\n",
    "    y_end = min(noise.shape[1] - y, block.shape[1])\n",
    "    y_start = max(0, -y)\n",
    "\n",
    "    noise[max(0, x):x+block.shape[0], max(0, y):y+block.shape[1]] = block[x_start:x_end,y_start:y_end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def apply_noise(image, coords):\n",
    "    return get_random_noise_image(image, coords, noise_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_feature_points(f):\n",
    "    f = f.replace('image', 'features')\n",
    "    return np.load(f)\n",
    "\n",
    "def get_landmark_points(f):   \n",
    "    f = f.replace('image', 'landmarks')\n",
    "    return np.load(f)\n",
    "\n",
    "def image_read(f):\n",
    "    return np.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Data Pre-Processing</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "images_by_class = [[(image_read(f), get_landmark_points(f), get_feature_points(f)) \n",
    "                    for f in x[1]] for x in face_to_file_paths_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create reflection with reflected coordinates\n",
    "for i in range(len(images_by_class)):\n",
    "    for j in range(len(images_by_class[i])):\n",
    "        im, landmarks, features = images_by_class[i][j]\n",
    "        new_features = [(im.shape[1] - p[0], p[1]) for p in features]\n",
    "        new_landmarks = [(im.shape[1] - p[0], p[1]) for p in landmarks]\n",
    "        new_landmarks[:3], new_landmarks[6:] = new_landmarks[6:], new_landmarks[:3]\n",
    "        images_by_class[i].append((reflection(im), new_landmarks, new_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Doubling the images and applying random gaussian noise\n",
    "for i in range(len(images_by_class)):\n",
    "    images_by_class[i] += images_by_class[i][:]\n",
    "    images_by_class[i] = [(apply_noise(im, features), landmarks) \n",
    "                          for im, landmarks, features in images_by_class[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get image of size 165x120, similar to the paper\n",
    "for i in range(len(images_by_class)):\n",
    "    images_by_class[i] = [get_image_window(im, input_shape, landmarks[4])\n",
    "                          for im, landmarks in images_by_class[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((165, 120), 26684)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter([image.shape\n",
    "            for images in images_by_class \n",
    "            for image in images if image.shape == input_shape\n",
    "            ]).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = [image \n",
    "            for images in images_by_class \n",
    "            for image in images \n",
    "            if image.shape == input_shape\n",
    "            ]\n",
    "y_train = [images[0] \n",
    "            for images in enumerate(images_by_class) \n",
    "            for image in images[1] \n",
    "            if image.shape == input_shape\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "zipped = np.array(zip(X_train, y_train))\n",
    "np.random.shuffle(zipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = np.array([x[0] for x in zipped])\n",
    "y_train = np.array([x[1] for x in zipped])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0], 1, input_shape[0], input_shape[1])\n",
    "Y_train = np_utils.to_categorical(y_train, len(images_by_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Training and Validation</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def NN2(shape, nb_classes, nb_fc6):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Layer 1\n",
    "    model.add(Convolution2D(64, 3, 3, activation='relu', input_shape=shape))\n",
    "    model.add(Convolution2D(128, 3, 3, activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    # Layer 2\n",
    "    model.add(Convolution2D(64, 3, 3, activation='relu'))\n",
    "    model.add(Convolution2D(128, 3, 3, activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    # Layer 3\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(128, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(128, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    #Layer 4\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    #Layer 5\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(256, 3, 3))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(AveragePooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(nb_fc6))\n",
    "    model.add(Dense(nb_classes, activation='softmax'))\n",
    "   \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_input_shape = (1, input_shape[0], input_shape[1])\n",
    "nb_fc6 = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/keras/models.py:392: UserWarning: The \"show_accuracy\" argument is deprecated, instead you should pass the \"accuracy\" metric to the model at compile time:\n",
      "`model.compile(optimizer, loss, metrics=[\"accuracy\"])`\n",
      "  warnings.warn('The \"show_accuracy\" argument is deprecated, '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22681 samples, validate on 4003 samples\n",
      "Epoch 1/10\n",
      "22681/22681 [==============================] - 739s - loss: 5.9070 - acc: 0.0703 - val_loss: 5.7301 - val_acc: 0.0862\n",
      "Epoch 2/10\n",
      "22681/22681 [==============================] - 739s - loss: 5.7608 - acc: 0.0780 - val_loss: 5.6625 - val_acc: 0.0862\n",
      "Epoch 3/10\n",
      "22681/22681 [==============================] - 739s - loss: 5.6310 - acc: 0.0808 - val_loss: 5.3689 - val_acc: 0.0989\n",
      "Epoch 4/10\n",
      "22681/22681 [==============================] - 739s - loss: 5.1812 - acc: 0.1073 - val_loss: 4.7195 - val_acc: 0.1566\n",
      "Epoch 5/10\n",
      "22681/22681 [==============================] - 740s - loss: 4.4595 - acc: 0.1895 - val_loss: 3.7226 - val_acc: 0.2928\n",
      "Epoch 6/10\n",
      "22681/22681 [==============================] - 741s - loss: 3.3033 - acc: 0.3371 - val_loss: 2.5481 - val_acc: 0.4667\n",
      "Epoch 7/10\n",
      "22681/22681 [==============================] - 741s - loss: 2.0065 - acc: 0.5407 - val_loss: 1.5800 - val_acc: 0.6273\n",
      "Epoch 8/10\n",
      "22681/22681 [==============================] - 739s - loss: 1.0619 - acc: 0.7278 - val_loss: 0.9733 - val_acc: 0.7624\n",
      "Epoch 9/10\n",
      "22681/22681 [==============================] - 739s - loss: 0.5104 - acc: 0.8586 - val_loss: 0.5128 - val_acc: 0.8776\n",
      "Epoch 10/10\n",
      "22681/22681 [==============================] - 739s - loss: 0.2694 - acc: 0.9198 - val_loss: 0.3369 - val_acc: 0.9178\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f149455a5d0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NN2(model_input_shape, num_people, nb_fc6)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "model.fit(X_train, Y_train, batch_size=32, nb_epoch=10, \n",
    "        show_accuracy=True, verbose=1, shuffle=True, validation_split=.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "json_string = model.to_json()\n",
    "open('models/CNN-H1_epoch-10.json', 'w').write(json_string)\n",
    "model.save_weights('models/CNN-H1_epoch-10.h5', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22681 samples, validate on 4003 samples\n",
      "Epoch 1/10\n",
      "22681/22681 [==============================] - 739s - loss: 0.0682 - acc: 0.9801 - val_loss: 0.2050 - val_acc: 0.9598\n",
      "Epoch 2/10\n",
      "  384/22681 [..............................] - ETA: 693s - loss: 0.0294 - acc: 0.9922"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=SGD(lr=0.001), metrics=['accuracy'])\n",
    "model.fit(X_train, Y_train, batch_size=32, nb_epoch=10,\n",
    "        show_accuracy=True, verbose=1, shuffle=True, validation_split=.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "json_string = model.to_json()\n",
    "open('models/CNN-H1_epoch-20.json', 'w').write(json_string)\n",
    "model.save_weights('models/CNN-H1_epoch-20.h5', overwrite=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
