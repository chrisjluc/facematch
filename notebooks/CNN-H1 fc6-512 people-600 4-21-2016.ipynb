{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>CNN-H1 4-21-2016</h1>\n",
    "\n",
    "<strong>Abstract</strong>\n",
    "Implementing the CNN-H1 using NN2 described in the paper: http://arxiv.org/pdf/1509.00244v1.pdf. \n",
    "\n",
    "<strong>Implementation</strong>\n",
    "<ul>\n",
    "<li>Narrowed the width of faces to focus on the face, cut out background</li>\n",
    "<li>Validation split of 15%</li>\n",
    "<li>fc6 layer has a length of 512</li>\n",
    "<li>Noise with width 15</li>\n",
    "<li>Number of people is 600</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: GRID K520 (CNMeM is enabled with initial size: 98.0% of memory, CuDNN 3007)\n"
     ]
    }
   ],
   "source": [
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import fnmatch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "\n",
    "from skimage import io\n",
    "from skimage.color import rgb2grey\n",
    "from skimage.transform import resize\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D, AveragePooling2D\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import model_from_json\n",
    "\n",
    "np.random.seed(123456)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = '../data/lfw_cropped'\n",
    "\n",
    "img_rows_load, img_cols_load = 160, 160\n",
    "img_rows, img_cols = 160, 120\n",
    "noise_width = 15\n",
    "num_people = 600"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Loading Files</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_face_to_file_path_dict():\n",
    "    face_to_file_paths_dict = {}\n",
    "    \n",
    "    for root, dirnames, filenames in os.walk(data_path):\n",
    "        for dirname in dirnames:\n",
    "            if dirname not in face_to_file_paths_dict:\n",
    "                face_to_file_paths_dict[dirname] = []\n",
    "            directory_path = os.path.join(data_path, dirname)\n",
    "            for filename in os.listdir(directory_path):\n",
    "                if filename.endswith(\".jpg\"):\n",
    "                    face_to_file_paths_dict[dirname].append(os.path.join(directory_path, filename))\n",
    "                            \n",
    "    return face_to_file_paths_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_face_to_file_paths_descending_list(face_to_file_paths_dict):\n",
    "    return sorted(face_to_file_paths_dict.items(), key=lambda x: len(x[1]), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "face_to_file_paths_dict = get_face_to_file_path_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "face_to_file_paths_list = get_face_to_file_paths_descending_list(face_to_file_paths_dict)[:num_people]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Data Pre-Processing</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_to_facial_feature_points = {}\n",
    "def get_facial_feature_points(f):\n",
    "    if f not in file_to_facial_feature_points:\n",
    "        file_to_facial_feature_points[f] = np.load(f)\n",
    "    return file_to_facial_feature_points[f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def image_read(f):\n",
    "    return rgb2grey(io.imread(f))\n",
    "    # return resize(rgb2grey(io.imread(f)), (img_rows_load, img_cols_load))\n",
    "\n",
    "def reflection(image):\n",
    "    return np.array([list(reversed(row)) for row in image])\n",
    "\n",
    "def partition(image, top_left, rows, cols):\n",
    "    return np.array([row[top_left[1]:top_left[1] + cols] for row in image[top_left[0]:top_left[0] + rows]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scale_coords(coords, resize_shape, original_shape):\n",
    "    scale = np.array(resize_shape).astype(float) / np.array(original_shape)\n",
    "    coords[:,0] = coords[:,0] * scale[1]\n",
    "    coords[:,1] = coords[:,1] * scale[0]\n",
    "    return coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_random_noise_image(image, coords, width):\n",
    "    \"\"\"\n",
    "    Apply random gaussian generated values\n",
    "    and distribute them on gaussian distributed square\n",
    "    centered on the coordinates passed in for the image\n",
    "    \"\"\"\n",
    "    \n",
    "    noise = np.zeros((image.shape[0], image.shape[1]))\n",
    "    for coord in coords:\n",
    "        # Convert coordinates to rows / columns\n",
    "        apply_noise_at_point(coord[1], coord[0], noise, width)\n",
    "    return np.clip(image + noise, 0, 1)\n",
    "\n",
    "def apply_noise_at_point(x, y, noise, width):\n",
    "    \"\"\"\n",
    "    Generate a block with a single random value placed at the center\n",
    "    Apply the Gaussian filter with std of 4\n",
    "    Place it on the noise array at the appropriate coordinates\n",
    "    \n",
    "    x represents the rows\n",
    "    y represents the cols\n",
    "    \"\"\"\n",
    "    \n",
    "    block = np.zeros((width, width))\n",
    "    block[width / 2, width / 2] = np.random.normal()\n",
    "    block = gaussian_filter(block, sigma=4)\n",
    "\n",
    "    x -= width / 2\n",
    "    y -= width / 2\n",
    "    \n",
    "    x_end = min(noise.shape[0] - x, block.shape[0])\n",
    "    x_start =  max(0, -x)\n",
    "\n",
    "    y_end = min(noise.shape[1] - y, block.shape[1])\n",
    "    y_start = max(0, -y)\n",
    "\n",
    "    noise[max(0, x):x+block.shape[0], max(0, y):y+block.shape[1]] = block[x_start:x_end,y_start:y_end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def apply_noise(image, coords):\n",
    "    return get_random_noise_image(image, coords, noise_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "images_by_class = [[(image_read(f), get_facial_feature_points(os.path.splitext(f)[0] + '.npy')) \n",
    "                    for f in x[1]] for x in face_to_file_paths_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Resize images and scale coordinates\n",
    "for i in range(len(images_by_class)):\n",
    "    images_by_class[i] = [(\n",
    "                  resize(im, (img_rows_load, img_cols_load)),\n",
    "                  scale_coords(coords, (img_rows_load, img_cols_load), im.shape)\n",
    "              ) \n",
    "              for im, coords in images_by_class[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create reflection with reflected coordinates\n",
    "for i in range(len(images_by_class)):\n",
    "    for j in range(len(images_by_class[i])):\n",
    "        im, coords = images_by_class[i][j]\n",
    "        new_coords = [(im.shape[1] - coord[0], coord[1]) for coord in coords]\n",
    "        images_by_class[i].append((reflection(im), new_coords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Doubling the images and applying random gaussian noise\n",
    "for i in range(len(images_by_class)):\n",
    "    images_by_class[i] += images_by_class[i][:]\n",
    "    images_by_class[i] = [apply_noise(im, coords) for im, coords in images_by_class[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Partion images to 160x120, similar to the paper\n",
    "for i in range(len(images_by_class)):\n",
    "    images_by_class[i] = [partition(im, (0, 20), img_rows, img_cols) for im in images_by_class[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = np.array([image for images in images_by_class for image in images])\n",
    "y_train = np.array([images[0] for images in enumerate(images_by_class) for image in images[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "zipped = np.array(zip(X_train, y_train))\n",
    "np.random.shuffle(zipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = np.array([x[0] for x in zipped])\n",
    "y_train = np.array([x[1] for x in zipped])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0], 1, img_rows, img_cols)\n",
    "Y_train = np_utils.to_categorical(y_train, len(images_by_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Training and Validation</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def NN2(input_shape, nb_classes, nb_fc6):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Layer 1\n",
    "    model.add(Convolution2D(64, 3, 3, activation='relu', input_shape=input_shape))\n",
    "    model.add(Convolution2D(128, 3, 3, activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    # Layer 2\n",
    "    model.add(Convolution2D(64, 3, 3, activation='relu'))\n",
    "    model.add(Convolution2D(128, 3, 3, activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    # Layer 3\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(128, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(128, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    #Layer 4\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    #Layer 5\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(256, 3, 3))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(AveragePooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(nb_fc6))\n",
    "    model.add(Dense(nb_classes, activation='softmax'))\n",
    "   \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_shape = (1, img_rows, img_cols)\n",
    "nb_fc6 = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22705 samples, validate on 4007 samples\n",
      "Epoch 1/10\n",
      "22705/22705 [==============================] - 708s - loss: 5.8829 - acc: 0.0737 - val_loss: 5.7242 - val_acc: 0.0849\n",
      "Epoch 2/10\n",
      "22705/22705 [==============================] - 708s - loss: 5.7676 - acc: 0.0782 - val_loss: 5.7430 - val_acc: 0.0849\n",
      "Epoch 3/10\n",
      "22705/22705 [==============================] - 708s - loss: 5.7491 - acc: 0.0782 - val_loss: 5.6880 - val_acc: 0.0849\n",
      "Epoch 4/10\n",
      "22705/22705 [==============================] - 708s - loss: 5.6747 - acc: 0.0801 - val_loss: 5.5329 - val_acc: 0.0881\n",
      "Epoch 5/10\n",
      "22705/22705 [==============================] - 708s - loss: 5.4540 - acc: 0.0895 - val_loss: 5.1628 - val_acc: 0.1188\n",
      "Epoch 6/10\n",
      "22705/22705 [==============================] - 708s - loss: 4.9887 - acc: 0.1342 - val_loss: 4.6655 - val_acc: 0.1789\n",
      "Epoch 7/10\n",
      "22705/22705 [==============================] - 708s - loss: 4.0838 - acc: 0.2366 - val_loss: 3.6003 - val_acc: 0.3127\n",
      "Epoch 8/10\n",
      "22705/22705 [==============================] - 709s - loss: 2.9421 - acc: 0.3826 - val_loss: 2.4707 - val_acc: 0.4637\n",
      "Epoch 9/10\n",
      "22705/22705 [==============================] - 708s - loss: 1.8464 - acc: 0.5689 - val_loss: 1.8224 - val_acc: 0.5830\n",
      "Epoch 10/10\n",
      "22705/22705 [==============================] - 709s - loss: 0.9991 - acc: 0.7424 - val_loss: 0.9169 - val_acc: 0.7796\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8e8031e210>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NN2(input_shape, num_people, nb_fc6)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd')\n",
    "model.fit(X_train, Y_train, batch_size=32, nb_epoch=10, \n",
    "        show_accuracy=True, verbose=1, shuffle=True, validation_split=.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "json_string = model.to_json()\n",
    "open('models/CNN-H1_fc6-512_people-600_epoch-10.json', 'w').write(json_string)\n",
    "model.save_weights('models/CNN-H1_fc6-512_people-600_epoch-10.h5', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights('models/CNN-H1_fc6-512_people-600_epoch-10.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22705 samples, validate on 4007 samples\n",
      "Epoch 1/10\n",
      "22705/22705 [==============================] - 709s - loss: 0.2460 - acc: 0.9307 - val_loss: 0.4772 - val_acc: 0.8944\n",
      "Epoch 2/10\n",
      "22705/22705 [==============================] - 709s - loss: 0.1460 - acc: 0.9594 - val_loss: 0.4088 - val_acc: 0.9104\n",
      "Epoch 3/10\n",
      "22705/22705 [==============================] - 709s - loss: 0.1077 - acc: 0.9684 - val_loss: 0.3652 - val_acc: 0.9281\n",
      "Epoch 4/10\n",
      "22705/22705 [==============================] - 709s - loss: 0.0802 - acc: 0.9778 - val_loss: 0.3355 - val_acc: 0.9364\n",
      "Epoch 5/10\n",
      "22705/22705 [==============================] - 709s - loss: 0.0685 - acc: 0.9796 - val_loss: 0.3314 - val_acc: 0.9394\n",
      "Epoch 6/10\n",
      "22705/22705 [==============================] - 709s - loss: 0.0545 - acc: 0.9841 - val_loss: 0.3211 - val_acc: 0.9441\n",
      "Epoch 7/10\n",
      "22705/22705 [==============================] - 709s - loss: 0.0468 - acc: 0.9864 - val_loss: 0.3051 - val_acc: 0.9496\n",
      "Epoch 8/10\n",
      "22705/22705 [==============================] - 709s - loss: 0.0429 - acc: 0.9877 - val_loss: 0.3253 - val_acc: 0.9466\n",
      "Epoch 9/10\n",
      "22705/22705 [==============================] - 709s - loss: 0.0376 - acc: 0.9888 - val_loss: 0.3041 - val_acc: 0.9498\n",
      "Epoch 10/10\n",
      "22705/22705 [==============================] - 709s - loss: 0.0369 - acc: 0.9892 - val_loss: 0.3041 - val_acc: 0.9528\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8c7ef6ead0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=SGD(lr=0.001))\n",
    "model.fit(X_train, Y_train, batch_size=32, nb_epoch=10,\n",
    "        show_accuracy=True, verbose=1, shuffle=True, validation_split=.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "json_string = model.to_json()\n",
    "open('models/CNN-H1_fc6-512_people-600_epoch-20.json', 'w').write(json_string)\n",
    "model.save_weights('models/CNN-H1_fc6-512_people-600_epoch-20.h5', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22705 samples, validate on 4007 samples\n",
      "Epoch 1/10\n",
      "22705/22705 [==============================] - 709s - loss: 0.0339 - acc: 0.9898 - val_loss: 0.3271 - val_acc: 0.9476\n",
      "Epoch 2/10\n",
      "22705/22705 [==============================] - 709s - loss: 0.0284 - acc: 0.9922 - val_loss: 0.3107 - val_acc: 0.9493\n",
      "Epoch 3/10\n",
      "22705/22705 [==============================] - 709s - loss: 0.0281 - acc: 0.9915 - val_loss: 0.3079 - val_acc: 0.9511\n",
      "Epoch 4/10\n",
      "22705/22705 [==============================] - 709s - loss: 0.0245 - acc: 0.9930 - val_loss: 0.3105 - val_acc: 0.9521\n",
      "Epoch 5/10\n",
      "22705/22705 [==============================] - 709s - loss: 0.0236 - acc: 0.9932 - val_loss: 0.3105 - val_acc: 0.9538\n",
      "Epoch 6/10\n",
      "22705/22705 [==============================] - 709s - loss: 0.0212 - acc: 0.9938 - val_loss: 0.3264 - val_acc: 0.9543\n",
      "Epoch 7/10\n",
      "22705/22705 [==============================] - 709s - loss: 0.0194 - acc: 0.9948 - val_loss: 0.3183 - val_acc: 0.9538\n",
      "Epoch 8/10\n",
      "22705/22705 [==============================] - 709s - loss: 0.0189 - acc: 0.9939 - val_loss: 0.3282 - val_acc: 0.9501\n",
      "Epoch 9/10\n",
      "22705/22705 [==============================] - 709s - loss: 0.0184 - acc: 0.9948 - val_loss: 0.3286 - val_acc: 0.9506\n",
      "Epoch 10/10\n",
      "22705/22705 [==============================] - 709s - loss: 0.0175 - acc: 0.9953 - val_loss: 0.3277 - val_acc: 0.9531\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8c82b12290>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=SGD(lr=0.001))\n",
    "model.fit(X_train, Y_train, batch_size=32, nb_epoch=10,\n",
    "        show_accuracy=True, verbose=1, shuffle=True, validation_split=.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "json_string = model.to_json()\n",
    "open('models/CNN-H1_fc6-512_people-600_epoch-30.json', 'w').write(json_string)\n",
    "model.save_weights('models/CNN-H1_fc6-512_people-600_epoch-30.h5', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
